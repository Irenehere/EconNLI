{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SFT Llama for event extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import torch\n",
    "from transformers import LlamaTokenizer,LlamaForCausalLM,TrainingArguments\n",
    "tqdm.pandas()\n",
    "\n",
    "df_event_extraction = pd.read_csv('../Corpus/event_extraction_human_annotation.csv')\n",
    "\n",
    "ds_train = datasets.Dataset.from_pandas(df_event_extraction[:100])\n",
    "ds_test = datasets.Dataset.from_pandas(df_event_extraction[100:])\n",
    "\n",
    "model_name = '../models/Llama-2-13b-chat-hf'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0:\"15GB\", 1:\"15GB\", 2:\"15GB\", 3:\"15GB\"}, \n",
    ")\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['sentence'])):\n",
    "        text = f\"### Question: An event is defined as somebody/something has some actions/states.\\\n",
    "List all events can be infered from the given sentence. \\\n",
    "If no event exists, simply respond 'no event.' \\\n",
    "The sentence: {example['sentence'][i]} \\n \\\n",
    "### Answer: {example['human_label'][i]}\"+tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template_with_context = \"\\n ### Answer:\" \n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  \n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/tmp\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=12,\n",
    "    learning_rate=0.00005,\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    max_seq_length=512,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"../models/llama-2-13b-hf-for-event-extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event extraction on Wiki corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import LlamaTokenizer,LlamaForCausalLM\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "corpus_path = '../Corpus/Wiki_Corpus'\n",
    "model_name = \"../models/llama-2-13b-hf-for-event-extraction\"\n",
    "prompt = \"### Question: An event is defined as somebody/something has some actions/states.\\\n",
    "List all events can be infered from the given sentence. If no event exists, simply respond 'no event.' \\\n",
    "The sentence: {} \\n ### Answer:\"\n",
    "\n",
    "if not os.path.exists(\"../Corpus/llama_labeled_events\"):\n",
    "    os.mkdir(\"../Corpus/llama_labeled_events\")\n",
    "\n",
    "corpus_file_list = os.listdir(corpus_path)\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0:\"15GB\", 1:\"15GB\", 2:\"15GB\", 3:\"15GB\"}, \n",
    ")\n",
    "\n",
    "def prompt_llama(model, tokenizer, prompt):\n",
    "    inputs =  tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=100)\n",
    "        outputs_string = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "        outputs_pred = outputs_string.split(\"### Answer:\")[1].strip().split('### Question')[0]\n",
    "    return outputs_pred \n",
    "    \n",
    "for i in tqdm(range(len(corpus_file_list))):\n",
    "    answer_list = []\n",
    "    with open(os.path.join(corpus_path,corpus_file_list[i]),'r') as f:\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            if len(line.split(\" \"))>30:\n",
    "                sentence_list = line.split('.')\n",
    "                for sentence in sentence_list:\n",
    "                    if len(sentence)<10:\n",
    "                        continue\n",
    "                    full_prompt = prompt.format(sentence)\n",
    "                    outputs_pred = prompt_llama(model, tokenizer, full_prompt)\n",
    "                    answer_list.append({'sentence':sentence,'llama_labeled_events':outputs_pred})\n",
    "                    print(sentence)\n",
    "                    print(outputs_pred)\n",
    "                    print(\"-\")\n",
    "            line = f.readline()\n",
    "    df = pd.DataFrame(answer_list)\n",
    "    df.to_csv(os.path.join(\"../Corpus/llama_labeled_events\",corpus_file_list[i].split('.')[0]+'.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconReasoningDataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
