{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizerFast, LlamaForCausalLM\n",
    "from sklearn.metrics import classification_report\n",
    "from peft import PeftModel\n",
    "\n",
    "from Eval_utils import *\n",
    "\n",
    "df_train = pd.read_csv(\"../Dataset/EconNLI_train.csv\")\n",
    "df_test = pd.read_csv(\"../Dataset/EconNLI_test.csv\" )\n",
    "#shuffle\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "df_train = df_train.rename(columns={\"ChatGPT_label\":\"label\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21406de8",
   "metadata": {},
   "source": [
    "## ICL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8b2f8",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ICL_results_from_llama(df_train, df_test, model, tokenizer):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    \n",
    "    for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        # sample ICL examples\n",
    "        icl_pos = df_train[df_train[\"label\"]==1].sample(1)\n",
    "        icl_neg = df_train[df_train[\"label\"]==0].sample(1)\n",
    "\n",
    "        topic = row[1][\"wiki_page\"]\n",
    "        premise = row[1][\"cause\"]\n",
    "        hypothesis = row[1][\"effect\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        prompt = f\"Conduct inference on economic events. We provide a premise and a hypothesis, both of them are economical events. Infer \\\n",
    "whether the premise can cause the happening of the hypothesis. Only answer 'Yes' or 'No'. \\\n",
    "Here are some examples: premise: {icl_pos['cause'].iloc[0]}, hypothesis: {icl_pos['effect'].iloc[0]}, answer:Yes \\n \\\n",
    "premise: {icl_neg['cause'].iloc[0]}, hypothesis: {icl_neg['effect'].iloc[0]}, answer:No \\n \\\n",
    "Conduct inference on the following premise and hypothesis: premise: {premise}, hypothesis: {hypothesis}, answer:\"\n",
    "        \n",
    "        model_answer = prompt_llama_like_model(prompt,model,tokenizer,max_new_tokens =3 )\n",
    "        prediction = model_answer.split(\"answer:\")[1].strip()\n",
    "        \n",
    "        if row[0]<10:\n",
    "            print(\"prompt: \",prompt)\n",
    "            print(\"prediction: {}\".format(prediction))\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "        if 'yes' in prediction.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(1)\n",
    "            y_true.append(label)\n",
    "        elif 'no' in prediction.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(0)\n",
    "            y_true.append(label)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Remove comments on your desired model\n",
    "\n",
    "# LLAMA2-7B-chat\n",
    "# model_name = \"../llama/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={2:\"24GB\",3:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LLAMA2-13B-chat\n",
    "# model_name = \"../llama/Llama-2-13b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={2:\"24GB\",3:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FINMA\n",
    "# model_name = \"ChanceFocus/finma-7b-nlp\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={0:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name,unk_token =\"<s>\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# #Alpaca\n",
    "# model_name = \"../llama/alpaca-7b/\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={0:\"24GB\",1:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "y_true, y_pred = get_ICL_results_from_llama(df_train, df_test, model, tokenizer)\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "\n",
    "with open(\"results/LLM_results.txt\",\"a\") as f:\n",
    "    f.write(model_name+\", ICL \\n\")\n",
    "    f.write(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "    f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73527ac",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9572923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for MODEL_NAME in [\"ChatGPT\",\"GPT4\"]:\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        # sample ICL examples\n",
    "        icl_pos = df_train[df_train[\"label\"]==1].sample(1)\n",
    "        icl_neg = df_train[df_train[\"label\"]==0].sample(1)\n",
    "\n",
    "        topic = row[1][\"wiki_page\"]\n",
    "        premise = row[1][\"cause\"]\n",
    "        hypothesis = row[1][\"effect\"]\n",
    "        label = row[1][\"label\"]\n",
    "\n",
    "        prompt = f\"Conduct inference on economic events. We provide a premise and a hypothesis, both of them are economical events. Infer \\\n",
    "whether the premise can cause the happening of the hypothesis. Only answer 'Yes' or 'No'. \\\n",
    "Here are some examples: premise: {icl_pos['cause'].iloc[0]}, hypothesis: {icl_pos['effect'].iloc[0]}, answer:Yes; \\\n",
    "premise: {icl_neg['cause'].iloc[0]}, hypothesis: {icl_neg['effect'].iloc[0]}, answer:No; \\\n",
    "Conduct inference on the following premise and hypothesis: premise: {premise}, hypothesis: {hypothesis}, answer:\"\n",
    "\n",
    "        \n",
    "        if MODEL_NAME == \"ChatGPT\":\n",
    "            prediction = prompt_chatgpt_with_backoff(prompt.format(premise, hypothesis))\n",
    "        elif MODEL_NAME == \"GPT4\":\n",
    "            prediction = prompt_gpt4_with_backoff(prompt.format(premise, hypothesis))\n",
    "        \n",
    "        if row[0]<10:\n",
    "            print(\"prompt: \",prompt)\n",
    "            print(\"prediction: {}\".format(prediction))\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "        if 'yes' in prediction.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(1)\n",
    "            y_true.append(label)\n",
    "        elif 'no' in prediction.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(0)\n",
    "            y_true.append(label)\n",
    "\n",
    "    print(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "    with open(\"results/LLM_results.txt\", \"a\") as f:\n",
    "        f.write(MODEL_NAME+\", ICL \\n\")\n",
    "        f.write(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc04e2",
   "metadata": {},
   "source": [
    "## COT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c536e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_prompt = '''\n",
    "### Question: Conduct inference on economic events. We provide a premise and a hypothesis,\\\n",
    "both of them are economic events. Infer whether the premise can cause the hypothesis to happen. \\\n",
    "Write the reasoning chain on the first line, and summarize the answer as 'Yes' or 'No' in the second line. \\\n",
    "premise: demand increases, hypothesis: price increases. \\n \\\n",
    "### Answer: When demand for a product or service increases, more people want to buy it. \\\n",
    "This creates a situation where there are more buyers than available supply, \\\n",
    "which leads to an increase in competition among buyers. \\\n",
    "As a result, sellers can raise their prices because they know that buyers are willing to pay more to get the product or service they want.\\n\\\n",
    "Yes.\\n\\\n",
    "\n",
    "### Question: Conduct inference on economic events. We provide a premise and a hypothesis,\\\n",
    "both of them are economic events. Infer whether the premise can cause the hypothesis to happen. \\\n",
    "Write the reasoning chain on the first line, and summarize the answer as 'Yes' or 'No' in the second line. \\\n",
    "premise: government borrowing creates higher demand for credit in the financial markets,\\\n",
    "hypothesis: interest rates decreases across the market. \\n \\\n",
    "### Answer:When the government borrows money, it creates higher demand for credit in the financial markets. \\\n",
    "This is because the government is competing with other borrowers for available funds, which can drive up interest rates. \\\n",
    "Therefore, it is unlikely that government borrowing would cause interest rates to decrease across the market. \\n \\\n",
    "No. \\n\\\n",
    "\n",
    "### Question: Conduct inference on economic events. We provide a premise and a hypothesis,\\\n",
    "both of them are economic events. Infer whether the premise can cause the hypothesis to happen. \\\n",
    "Write the reasoning chain on the first line, and summarize the answer as 'Yes' or 'No' in the second line. \\\n",
    "premise:{}, hypothesis: {} \\n \\\n",
    "\n",
    "### Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15085e1e",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_COT_results_from_llama(cot_prompt, df_test, model, tokenizer):\n",
    "    y_true = []\n",
    "    y_pred = [] \n",
    "    \n",
    "    for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        \n",
    "        topic = row[1][\"wiki_page\"]\n",
    "        premise = row[1][\"cause\"]\n",
    "        hypothesis = row[1][\"effect\"]\n",
    "        label = row[1][\"label\"]\n",
    "        \n",
    "        model_answer = prompt_llama_like_model(cot_prompt.format(premise,hypothesis), model,tokenizer,max_new_tokens = 200)\n",
    "        prediction = model_answer.split(\"### Answer:\")[3].split(\"### Question:\")[0].strip()\n",
    "        \n",
    "        if row[0]<10:\n",
    "#             print(\"prompt: \",cot_prompt)\n",
    "            print(\"model_output: {}\".format(model_answer))\n",
    "            print(\"prediction: {}\".format(prediction))\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "#         if 'Yes.' in prediction:   # LLAMA(7B,13B),Alpaca\n",
    "        if 'Yes' in prediction:   # PIXIU\n",
    "            y_pred.append(1)\n",
    "            y_true.append(label)\n",
    "#         elif 'No.' in prediction:    # LLAMA(7B,13B),Alpaca\n",
    "        if 'No' in prediction:   # PIXIU\n",
    "            y_pred.append(0)\n",
    "            y_true.append(label)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "# Remove comments on your desired model\n",
    "\n",
    "# LLAMA2-7B-chat\n",
    "# model_name = \"../llama/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={2:\"24GB\",3:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LLAMA2-13B-chat\n",
    "# model_name = \"../llama/Llama-2-13b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={2:\"24GB\",3:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# FINMA\n",
    "# model_name = \"ChanceFocus/finma-7b-nlp\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={0:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name,unk_token =\"<s>\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# #Alpaca\n",
    "# model_name = \"../llama/alpaca-7b/\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     max_memory={0:\"24GB\",1:\"24GB\"}, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "y_true, y_pred = get_COT_results_from_llama(cot_prompt, df_test, model, tokenizer)\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "\n",
    "with open(\"results/LLM_results.txt\",\"a\") as f:\n",
    "    f.write(model_name+\", COT \\n\")\n",
    "    f.write(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "    f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e166a4",
   "metadata": {},
   "source": [
    "### ChatGPT/GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d89ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for MODEL_NAME in [\"ChatGPT\",\"GPT4\"]:\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "        \n",
    "    for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        time.sleep(1)\n",
    "        topic = row[1][\"wiki_page\"]\n",
    "        premise = row[1][\"cause\"]\n",
    "        hypothesis = row[1][\"effect\"]\n",
    "        label = row[1][\"label\"]\n",
    "        if MODEL_NAME == \"ChatGPT\":\n",
    "            prediction = prompt_chatgpt_with_backoff(cot_prompt.format(premise, hypothesis))\n",
    "        elif MODEL_NAME == \"GPT4\":\n",
    "            prediction = prompt_gpt4_with_backoff(cot_prompt.format(premise, hypothesis))\n",
    "        \n",
    "        if row[0]<10:\n",
    "            print(cot_prompt.format(premise, hypothesis))\n",
    "            print(prediction)\n",
    "            print(\"======\")\n",
    "\n",
    "        if 'yes' in prediction.split('\\n')[-1].lower():\n",
    "            pred = 1\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(label)\n",
    "        elif 'no' in prediction.split('\\n')[-1].lower():\n",
    "            pred = 0\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(label)\n",
    "\n",
    "        if (pred==1 and label==0) or (pred==0 and label==1):\n",
    "            print(\"premise: {}, hypothesis: {}, label: {}\".format(premise, hypothesis, label))\n",
    "            print(\"prediction: {}\".format(prediction))\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "    print(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "    with open(\"results/LLM_results.txt\", \"a\") as f:\n",
    "        f.write(MODEL_NAME+\", COT \\n\")\n",
    "        f.write(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "        f.write(\"\\n\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconReasoningDataset",
   "language": "python",
   "name": "econreasoningdataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
