{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification,AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM, LlamaTokenizer\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score, classification_report\n",
    "from peft import LoraConfig\n",
    "from Eval_utils import *\n",
    "\n",
    "df_train = pd.read_csv(\"../Dataset/EconNLI_train.csv\")\n",
    "df_test = pd.read_csv(\"../Dataset/EconNLI_test.csv\" )\n",
    "#shuffle\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "df_train = df_train.rename(columns={\"ChatGPT_label\":\"label\"}) # we use ChatGPT's label for SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775180e",
   "metadata": {},
   "source": [
    "## BERT-like Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65cf32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"cause\"], examples[\"effect\"] ,truncation=True,max_length=512)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    prec = precision_score(y_true = labels, y_pred = predictions, average=None).tolist()\n",
    "    recall = recall_score(y_true = labels, y_pred = predictions, average=None).tolist()\n",
    "    f1 = f1_score(y_true = labels, y_pred = predictions, average=None).tolist()\n",
    "    mic_f1 = f1_score(y_true = labels, y_pred = predictions, average='micro')\n",
    "    acc = accuracy_score(y_true = labels, y_pred = predictions)\n",
    "    return {\"precision\": prec, \"recall\": recall, \"f1\": f1, \"micro_f1\":mic_f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(df_train, df_test, model, tokenizer,model_output_path):\n",
    "    df_train, df_val = np.split(df_train, [int(.9*len(df_train))])\n",
    "    dataset_train = datasets.Dataset.from_pandas(df_train)\n",
    "    dataset_val = datasets.Dataset.from_pandas(df_val)\n",
    "    dataset_test = datasets.Dataset.from_pandas(df_test)\n",
    "    tokenized_train = dataset_train.map(preprocess_function)\n",
    "    tokenized_val = dataset_val.map(preprocess_function)\n",
    "    tokenized_test = dataset_test.map(preprocess_function)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=20,\n",
    "        per_device_eval_batch_size=64,\n",
    "        num_train_epochs=3, \n",
    "        weight_decay=0.01,\n",
    "        do_train = True,\n",
    "        do_eval = True,\n",
    "        save_strategy = 'epoch',\n",
    "        save_total_limit=1,\n",
    "        evaluation_strategy = 'epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        overwrite_output_dir = True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    print(\"Results on test set:\")\n",
    "    res = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "    print(res)\n",
    "    return trainer, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f589a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for MODEL_NAME_OR_PATH in [\"bert-base-uncased\", \"roberta-base\", \"yiyanghkust/finbert-pretrain\", \"SALT-NLP/FLANG-BERT\",\"SALT-NLP/FLANG-ELECTRA\"]: \n",
    "    \n",
    "    MODEL_OUTPUT_PATH = \"models/FT_PLMs_\"+MODEL_NAME_OR_PATH\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_OR_PATH,num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "    for i in range(3):\n",
    "\n",
    "        trainer,pred = train_and_evaluate_model(df_train, df_test, model, tokenizer, MODEL_OUTPUT_PATH)\n",
    "\n",
    "        dataset_test = datasets.Dataset.from_pandas(df_test)\n",
    "        tokenized_test = dataset_test.map(preprocess_function)\n",
    "        res = trainer.predict(tokenized_test)\n",
    "\n",
    "        print(classification_report(y_pred = np.argmax(res[0],axis=1), y_true = res[1],digits=4))\n",
    "\n",
    "        with open('results/FT_PLMs_results.txt', 'a') as f:\n",
    "            f.write(MODEL_NAME_OR_PATH+\", run \"+str(i)+\"\\n\")\n",
    "            f.write(classification_report(y_pred = np.argmax(res[0],axis=1), y_true = res[1],digits=4))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9703d",
   "metadata": {},
   "source": [
    "## LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = np.split(df_train, [int(.9*len(df_train))])\n",
    "ds_train = datasets.Dataset.from_pandas(df_train)\n",
    "ds_val = datasets.Dataset.from_pandas(df_val)\n",
    "ds_test = datasets.Dataset.from_pandas(df_test)\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['sentence'])):\n",
    "        answer = \"Yes\" if example['label'][i]==1 else \"No\"\n",
    "        text = f\"### Question: Conduct inference on economic events. We provide a premise and a hypothesis,\\\n",
    "both of them are economical events. Infer \\\n",
    "whether the premise can cause the happening of the hypothesis. Only answer 'Yes' or 'No'. \\\n",
    "premise: {example['cause'][i]}, hypothesis: {example['effect'][i]}. \\\n",
    "\\n ### Answer: {answer} \" +tokenizer.eos_token\n",
    "        output_texts.append(text)\n",
    "#         if i==10:\n",
    "    print(output_texts[:10])\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def get_zero_shot_results_from_llama(df_test, model, tokenizer):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        prompt = f\"### Question: Conduct inference on economic events. We provide a premise and a hypothesis,\\\n",
    "both of them are economical events. Infer \\\n",
    "whether the premise can cause the happening of the hypothesis. Only answer 'Yes' or 'No'. \\\n",
    "premise: {row[1]['cause']}, hypothesis: {row[1]['effect']}. \\\n",
    "\\n ### Answer:\"\n",
    "        model_answer = prompt_llama_like_model(prompt,model,tokenizer,max_new_tokens = 3)\n",
    "        model_answer = model_answer.split(\"\\n ### Answer:\")[1].strip()\n",
    "        if row[0]<10:\n",
    "            print(prompt)\n",
    "            print(model_answer)\n",
    "        if \"yes\" in model_answer.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(1)\n",
    "            y_true.append(row[1]['label'])\n",
    "        elif \"no\" in model_answer.strip('\\n').split(\" \")[0].lower():\n",
    "            y_pred.append(0)\n",
    "            y_true.append(row[1]['label'])\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "\n",
    "for model_name in [\"../llama/Llama-2-7b-chat-hf\", \"../llama/Llama-2-13b-chat-hf\"]:\n",
    "    for run in range(3):\n",
    "        y_true, y_pred = None, None\n",
    "        trainer = None\n",
    "\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name,\n",
    "            device_map=\"auto\",                        \n",
    "            max_memory={0:\"24GB\",1:\"24GB\",2:\"24GB\",3:\"24GB\"}, \n",
    "            torch_dtype=torch.float16,\n",
    "            )\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name,)\n",
    "        tokenizer.pad_token = tokenizer.eos_token   \n",
    "\n",
    "        response_template = \"\\n ### Answer:\"\n",
    "        collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='models/model_name_'+str(run),\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=12,\n",
    "            learning_rate=0.00005,\n",
    "            logging_steps=10,\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            max_seq_length=512,\n",
    "            train_dataset=ds_train,\n",
    "            eval_dataset=ds_val,\n",
    "            peft_config=peft_config,\n",
    "            formatting_func=formatting_prompts_func,\n",
    "            data_collator=collator,\n",
    "\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        y_true, y_pred = get_zero_shot_results_from_llama(df_test, model, tokenizer)\n",
    "        print(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "        \n",
    "        with open(\"results/FT_PLMs_results.txt\",\"a\") as f:\n",
    "            f.write(model_name+\", SFT, run \"+str(run)+\" \\n\")\n",
    "            f.write(classification_report(y_true=y_true,y_pred=y_pred,digits=4))\n",
    "            f.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconReasoningDataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
